 %!TEX root = ../elaboration.tex
\chapter{Results}
\label{cha:results}

\epigraph{This chapter will present the results of the implemented system, divided in two main sections. The first will analyse the quality and availability of the data collected by the \gls{IMU} Sensors, and the second will analyse the performance of the game metrics, described in Section~\ref{sec:metrics}.}

\section{Data Availability and Quality}
\label{sec:resultsData}
The evaluation of the data will be separated in two parts: the availability and the quality of the data.
The availability will measure if all the raw data sent by the \gls{IMU} Sensors is received by the Raspberry Pi's, and what factors affect it.
The quality will measure the degree of truthfulness of the raw data measured by the \gls{IMU} Sensors.

\subsection{Data Availability}
To measure the availability of data, it is retrieved from the \gls{IMU} Sensors, at 50Hz, from the accelerometer and gyroscope. No computations are made on the data. This means that the Raspberry Pi should receive 50 samples of Raw Data in 1 second. Measuring how many samples are missing, we can calculate the percentage of lost samples.

This measurements will be made in an open space, changing 3 factors: the Raspberry Pi version (3B+ vs 4); the distance between the Raspberry Pi and the \gls{IMU} Sensors; the number of \gls{IMU} Sensors.

The results from the different versions of the Raspberry Pi are displayed in Figures \ref{fig:datalossrPi3} and \ref{fig:datalossrPi4}. Between the two versions there is a big discrepancy in the percentage of samples lost, right from the closest distance. While Raspberry Pi 3B+ presents values between 5 and 10 percent, Raspberry Pi 4 has values between 0 and 1 percent. From there, every distance has a higher sample loss registered in Raspberry Pi 3B+ than in Raspberry Pi 4. This has to do with the introduction of the new version of Bluetooth 5.0 in the newer version of the Raspberry Pi.

Increasing the number of devices doesn't seem to correlate with a bigger sample loss. Even though sometimes the highest number of devices has the higher percentage of lost samples, that doesn't always verifies.

However, increasing the distance increases, in the vast majority of the cases, the percentage of samples lost. In both graphs we see an increasing trend, no matter the version of the Raspberry Pi used or the number of devices connected.

There are some outliers, like the sudden decrease at 12 meters in Figure~\ref{fig:datalossrPi3}, or the peak of percentage of lost samples using 1 device, at 9 meters, in Figure~\ref{fig:datalossrPi4}. These could be caused by other bluetooth devices in the proximity, or other sources of noise, or the unreliable quality of the sensors. The interference of other devices wasn't studied in this work, but in a basketball court with hundreds of fans using a consumer technology like bluetooth, this could be a problem in terms of communication performance.

\begin{figure}[htpb]
    \centering
    \subcaptionbox{Percentage of data loss (Raspberry Pi 3B+)\label{fig:datalossrPi3}}%
    {\includegraphics[width=.5\textwidth]{/presentation/loss_3.pdf}}%
    \subcaptionbox{Percentage of data loss (Raspberry Pi 4)\label{fig:datalossrPi4}}%
    {\includegraphics[width=.5\textwidth]{/presentation/loss_4.pdf}}%
    \caption{Data avaiability}
    \label{fig:avaiability}
\end{figure}

\subsection{Data Quality}
Regarding data quality, the raw data from a still sensor in a leveled surface will be analysed. Figure \ref{fig:quality} shows the first 20 samples collected from the IMU used in this work, placed in a flat surface. The rest of the samples were omitted, since these are enough to illustrate the big picture.

As we can see by comparing the first values with the remaining, the values are quite different. This might be caused by residual values left by the previous session. The data from the first row should be discarded, as it doesn't correspond to reality.

In the accelerometer graph, the only axis that should have some value other than zero should be the $Z$ axis, and it should be the value of the force of gravity ($\approx 9,8 m/s^2$ ). In the example, the values of the $Z$ axis is lower than what it should be, and there is acceleration recorded in the $X$ and $Y$ axis, when there should be none, as the sensors are in a leveled surface. This may happen due to incorrect internal placement of the sensor inside its housing, leading to incorrect readings when the housing is leveled.

Regarding the gyroscope graph, as the sensor is still, all the axis should be zero, or close to zero. Yet, the gyroscope shows significant values inand $X$ and $Y$ axis (different than 0). The $Z$ axis of the gyroscope is the one that is closest to being zero.

One way of correcting these errors is by removing the bias of the gyroscope values. This can be done by collecting data from each axis of the gyroscope, while the \gls{IMU} sensor is still, and to average the values of each axis. This gives the average bias. Then, for each sample, this calculated average bias should be subtracted from the gyroscope readings. The algorithm used for tracking the players reccomends the use of this correction to the gyroscope values, as it improves the algorithm performance.

\begin{figure}[htpb]
    \centering
    \subcaptionbox{Accelerometer\label{fig:quality_acc}}%
    {\includegraphics[width=0.75\linewidth]{/presentation/quality_acc.pdf}}%
    \\
    \subcaptionbox{Gyroscope\label{fig:quality_gyr}}%
    {\includegraphics[width=0.75\linewidth]{/presentation/quality_gyr.pdf}}%
    \caption{Data Quality}
    \label{fig:quality}
\end{figure}


\section{Metrics Performance}
\label{sec:resultsMetrics}
This section will present the results obtained from testing the performance metric algorithms, presented in Section~\ref{sec:metrics}.

\subsection{Position Tracking and Distance}
\label{subsec:positionresults}
Position tracking and distance were tested by using a sensor placed in the foot, above the shoe, as detailed in Subsection~\ref{subsubsec:trajectories}.

Firstly, the calculation of distance was put to the test. The distance is calculated by measuring the distance between each coordinate, using the Euclidean distance. The first test performed was to find what was the distance calculated when the sensor was still. Figure~\ref{fig:stillDistance} shows the plot of data collected during 30 seconds, while the foot was still. As we can see, the displacement is very low, and the distance calculated is in the order of the millimeters.

\begin{figure}
    \centering
    \includegraphics[width=.8\textwidth]{stillDistance.pdf}
    \caption{Still Distance Plot}
    \label{fig:stillDistance}
\end{figure}

The second thing to test was the actual measurements of distance, compared to the real walked distance. For this, two sensors were worn, one in each foot, to compare the measurements between the two. It was asked to the subject to walk in a straight line, with varying distances: 6 meters, 12 meters and 18 meters.
Figures \ref{fig:direitoStill} and \ref{fig:esquerdoStill} show the plot of the 3 distances, with data collected from the right and the left foot, respectively. The right foot presents calculated distances of 6.94m, 13.69m and 23.35m, and the left foot presents distances of 7.09m, 15.15m and 22.00m.
As we can see by the plot, the further the subject walks, the greater is the error of the calculated distance. This has to do with the accumulated error of the sensor values. These errors are originated in gyroscope noise, and are then propagated through the heading and speed estimations, affecting the position estimations. The error in position estimation affects directly the distance calculation.

\begin{figure}[htbp]
    \centering
    \subcaptionbox{Right Foot - 6m\label{fig:direito6}}%
    {\includegraphics[width=0.33\linewidth]{peDireito6m.pdf}}%
    \subcaptionbox{Right Foot - 12m\label{fig:direito12}}%
    {\includegraphics[width=0.33\linewidth]{peDireito12m.pdf}}%
    \subcaptionbox{Right Foot - 18m\label{fig:direito18}}%
    {\includegraphics[width=0.33\linewidth]{peDireito18m.pdf}}%
    \caption{Right foot - Still}
    \label{fig:direitoStill}
\end{figure}

\begin{figure}[htbp]
    \centering
    \subcaptionbox{Left Foot - 6m\label{fig:esquerdo6}}%
    {\includegraphics[width=0.33\linewidth]{peEsquerdo6m.pdf}}%
    \subcaptionbox{Left  Foot - 12m\label{fig:esquerdo12}}%
    {\includegraphics[width=0.33\linewidth]{peEsquerdo12m.pdf}}%
    \subcaptionbox{Left  Foot - 18m\label{fig:esquerdo18}}%
    {\includegraphics[width=0.33\linewidth]{peEsquerdo18m.pdf}}%
    \caption{Left foot - Still}
    \label{fig:esquerdoStill}
\end{figure}

Secondly, the tracking of the subject position was tested. In this test, more complex routes were followed, and the subjects were asked not only to walk, but also to run, to test the response of the tracking algorithm.

The first test was to walk in a straight line and doing 90$^{\circ}$ right turns, in the office corridors. The route is illustrated in Figure~\ref{fig:caracolRoute}.

\begin{figure}
    \centering
    \includegraphics[width=.8\textwidth]{caracolOriginal.pdf}
    \caption{Positional Tracking First Test Route}
    \label{fig:caracolRoute}
\end{figure}

As we can see in Figure~\ref{fig:caracol}, the distance, that should sum up to 65 meters, was calculated as being 90 meters. The positional tracking of the subject also presents some deviations, especially right at the beginning. The subject was asked to stand still for a moment before he started walking, so that the sensor could be at rest. However, instead of plotting the route to the front as a straight line (which would correspond to reality), it plotted an angled line. Consequently, all the route was tilted. On the other hand, even though this initial detour, all the following turns seem to match the reality, showing the 90$^{\circ}$ turns made by the subject. 
\begin{figure}
    \centering
    \includegraphics[width=.8\textwidth]{Caracol.pdf}
    \caption{Positional Tracking First Test}
    \label{fig:caracol}
\end{figure}
The second test consisted in performing the route illustrated in Figure~\ref{fig:uoriginal}, while walking and then performing a mix of both: start the route walking, and after the turn, start running, to see how the algorithm performed.

\begin{figure}
    \centering
    \includegraphics[width=.8\textwidth]{uOriginal.pdf}
    \caption{Positional Tracking Second Test Route}
    \label{fig:uoriginal}
\end{figure}

In the first example, illustrated in Figure~\ref{fig:uoriginalAndar}, we can see the deviation of the algorithm, after about 10m, but the turns are well traced, and the second straight line is also well traced. If it wasn't for the detour, the plot would be near perfect.

\begin{figure}
    \centering
    \includegraphics[width=.8\textwidth]{uOriginal+andar.pdf}
    \caption{Positional Tracking Second Test - Walking}
    \label{fig:uoriginalAndar}
\end{figure}

In the second example, illustrated in Figure~\ref{fig:uoriginalAndarCorrer}, even though the first part of the route shows a well traced straight line and turn, when the run started (after the turn), the algorithm couldn't identify well the movement. The tracing is very imprecise, and the last turn wasn't identified. 

\begin{figure}
    \centering
    \includegraphics[width=.8\textwidth]{uOriginal+andarCorrer.pdf}
    \caption{Positional Tracking Second Test - Walking and Running}
    \label{fig:uoriginalAndarCorrer}
\end{figure}

This might be due to two factors: the rate at which data is being collected (50Hz) might be insufficient to detect a fast movement such as running, or the movements produced by a running activity might be out of the range of the accelerometer, which was set for -8g to +8g.

Basketball players spent most of their running, so the difficulty of this algorithm to recognize running movements is a drawback. However, this algorithm is promising and did provide good results when walking. Collecting data at a higher rate, increasing the sensibility of the sensor and improving the detection algorithm could be some steps that could be taken in order to achieve better results in the positional tracking of a subject when he is running.

\subsection{Steps}
For testing the Step counting algorithm, it was asked to 3 subjects to walk and run in a straight line, taking varying numbers of steps. As the sensor is placed in the right foot, the algorithm only detects half of the steps (the steps taken with the right foot). Therefore, to obtain the total amount of steps, the result calculated by the algorithm should be multiplied by two, taking into account that the doubled value can be off by one step (\textit{i.e.} if the movement started with the right foot, and 5 steps were given in total, ending with the right foot, the sensor placed in the right foot would register 3 steps. Doubling this value, we would have a total of 6 steps, which is wrong by one step.)

Figure \ref{fig:stepDetection} shows the results of the tests performed by the subjects. The blue bars show the number of steps taken while walking, and the green bars show the numbers of steps taken while running, in each session. The red area represents the number of steps detected. The percentage values are the percentage of correct step detections, performed by the algorithm.

As we can see, the algorithm performs very well, detecting all the steps taken by the foot where the sensor was placed, even when running. As discussed in Subsection~\ref{subsec:steps}, there is still place for improving the algorithm, but the results with the current version are very satisfying.

\begin{figure}
    \centering
    \includegraphics[width=.8\textwidth]{stepDetection.pdf}
    \caption{Step Detection}
    \label{fig:stepDetection}
\end{figure}

\subsection{Dribbles}
To test the dribble detection algorithm algorithm described in Subsection~\ref{subsec:dribbles}, it was asked to five subjects to perform a set of 20 dribbles, at three different speeds: slow ($\approx$90 dribbles per minute), medium ($\approx$120 dribbles per minute) and fast ($\approx$160 dribbles per minute). The dribbles were all performed in-place.

Figure~\ref{fig:dribbledetection} shows the percentage of correct number of dribble detections achieved by this algorithm. Each bar represents the number of dribbles performed by each subject, and the colors represent the speed of the dribbling: yellow for slow, green for medium and blue for fast. The red area represents the number of dribbles detected by the algorithm, and the percentage of correctly detected dribbles is shown in the bottom of each column, as a percentage.

The dribble detection algorithm can be wrong if it detects less or more dribbles than those that were really performed. The case of detecting more dribbles was only verified when dribbling fast.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{dribbleDetection.pdf}
    \caption{Dribble Detection}
    \label{fig:dribbledetection}
\end{figure}

By analyzing the results, we can see that slow dribbling had the worst results of all three. The most likely cause of this bigger error might be the frequency threshold to confirm that the movement is a dribble. However, if this threshold is lowered, simple arm or wrist movements could be identified as a dribble movement. A compromise must be made, between detecting all the dribble movements, but also detect wrong random movements, or to discard slow dribbling movements, but ensure that other arm movements aren't identified as dribbles. The latter approach was selected.

Identifying medium and fast dribbling achieved better results, with a percentage of correct dribble detections always between 90\% and 100\%.
In fast dribbling, the algorithm would sometimes detect more dribbles than those that were actually made, and in medium dribbling, the algorithm would detect one or two less dribbles than those that were actually made, maybe because of different hand movement when starting or stopping the movement.

\subsection{Jumps}
To collect jump data for the results phase, it was asked to three subjects to perform sets of ten jumps, with a rest of 1 or 2 seconds between each jump. The jumps were either done vertically, or with a small displacement to the front. The sensor was placed in the lower back of each subject, held in place with the help of an abdominal binder.

In Figure~\ref{fig:jumpdetection} we can see the results of the data collected. The blue bars show the number of jumps performed in each session, and the red area represents the number of jumps detected. The percentage values are the percentage of correct jump detections, performed by the algorithm. The average of the correct detections of this algorithm is 17\% percent. This is value is very low and unsatisfactory. 

\begin{figure}
    \centering
    \includegraphics[width=.8\textwidth]{jumpDetection.pdf}
    \caption{Jump Detection}
    \label{fig:jumpdetection}
\end{figure}

However, while collecting data, some particularities of the algorithm were detected, and could be improved in the future. For example, when two similar jumps were performed, with a rest time of about 2 seconds between them, sometimes the algorithm detected zero jumps, and sometimes it only detects one jump, but never the two jumps that were really performed.
This may have to do with the reset parameters of the algorithm not being well adjusted. This means that the algorithm is not identifying correctly the moment when the subject is at rest, thus not detecting all the jumps.

Another problem is the threshold from which the algorithm detects jumps. This threshold was set in development phase to separate running movements from jumps. As the jump is a slower movement than running, its frequency is also lower. While developing, the value set looked good, but when tested with other subjects, and after analyzing the results, we can conclude that the value can be increased, to detect more jumps. A drawback of this increase is the detection of unwanted movements, but it might be necessary, taking into account the poor performance of this algorithm.

\section{Overall System Analysis}
The current system implementation is comprised of three elements: the \gls{IMU} Sensors, provided by \gls{KBZ}, one Raspberry Pi 4, and a computer, acting as a Server and hosting the application, accessed through the browser.

As seen in the previous Sections, there were available two versions of the Raspberry: a Raspberry Pi 3, and a Raspberry Pi 4. It is advisable to use the Raspberry Pi 4, due to its improved capabilities in terms of communication, performing better in terms of data transmission and handling better the distance between the sensors and the Raspberry Pi.

In terms of the maximum numbers of devices supported by each Raspberry Pi, it's limits weren't tested, as there were no problems connecting all nine available \gls{IMU} Sensors.

However, to test the scalability of the developed prototype, and only as a proof of concept (even though there weren't available two Raspberry Pi's, neither enough number of \gls{IMU} Sensors to reach the maximum number of connections), the available Raspberry Pi 3 and Raspberry Pi 4 were both turned on and connected to the server, and the Sensors were forced to connect to one of them, with each Raspberry Pi handling a similar number of devices. The normal process of collecting data, processing the game metrics and sending them to the Server presented no problems, showing strong evidence that the developed system is capable of scaling.

Regarding the performance of the metrics algorithms, showed in the previous section, they weren't tested in parallel, with data from various players at the same time. Nonetheless, a simulation was made, collecting data from different \gls{IMU} Sensors, as if they were placed in different body parts of different players, although the sensors were still in the top of a desk. The collection of data, processing the data through the metrics algorithms and sending them to the Server went without problems. With this evidence we can believe that in a real life environment there would be no problems, but it can only be proven if tested.

Looking at the application, even though it's development isn't finished, it's possible to perform all the basic operations, such as add players and teams to the system, and manage the \gls{IMU} Sensors worn by the players, and the data collected and processed from the sensors can be visualized in real-time. As explained in Subsection~\ref{subsec:example}, the process of registering all the \gls{IMU} Sensors that each player is using is a bit intrusive to the practice or the game, and can be time-consuming. Other than that, the user experience of the application is  (in the case of the developed application, the coach) is very simple, and with a touch of a button the coach can "Start"{} and "Stop"{} the game (\textit{i.e.} start the collection and processing of data from the \gls{IMU} Sensors), and can track the player's performance in real time.

The biggest flaw of the developed system prototype is in the game metrics that had the poorest performance: jumps and position tracking, especially when running.
The jumping algorithm presented very bad results, detecting 17\% of the performed jumps on average. This algorithm used to detect this movement should be improved, and the use of more data (from different sensors) should be considered.
The tracking algorithm had more satisfactory results, but only when the subjects walked. When running, the algorithm couldn't identify the path taken by the subject. The algorithm should be adapted to recognize faster foot movements, so that it can also be able to track the position when running.
Unfortunately, there was no time to improve the algorithms these algorithms. It's improvement must be considered as future work, along with the introduction of new game metrics, like shoots and passes, or the implementation of more robust classification techniques, such as machine learning.